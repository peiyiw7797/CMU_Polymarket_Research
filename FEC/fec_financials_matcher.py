"""Match Polymarket markets to FEC candidates using the lookup CLI.

This script replaces the legacy FEC API calls with the local
``lookup.cli`` command-line utility. For each market row in
``polymarket_election_markets_closed_2y.csv`` it:

1. Builds one or more lookup queries from the `candidate_names` column.
2. Invokes ``python -m lookup.cli search <query> --json`` to retrieve
   potential candidate matches from the warehouse-backed lookup service.
3. Applies temporal and office filters derived from the market metadata.
4. Writes ``fec_financials_matched.csv`` with the original market rows
   duplicated for every matching candidate.

Prerequisites
-------------
* The lookup service database must be accessible and configured via the
  ``FEC_LOOKUP_DATABASE_URL`` environment variable.
* The Polymarket dataset with an ``office`` column must be generated by
  ``polymarket_market_collector.py``.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import re
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence

import pandas as pd


LOGGER = logging.getLogger("fec_matcher")
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")


DEFAULT_INPUT = Path("data/polymarket_election_markets_closed_2y.csv")
DEFAULT_OUTPUT = Path("fec_financials_matched.csv")
CLI_BASE = [sys.executable, "-m", "lookup.cli", "--json", "search"]
OFFICE_FILTER_SET = {"president", "senate", "house"}


@dataclass(frozen=True)
class CandidateResult:
    cand_id: str
    normalized_name: Optional[str]
    display_name: Optional[str]
    office: Optional[str]
    office_full: Optional[str]
    cycle: Optional[int]
    election_year: Optional[int]

    @property
    def name_for_output(self) -> Optional[str]:
        return self.normalized_name or self.display_name


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Match Polymarket markets to FEC candidates")
    parser.add_argument("--input", type=Path, default=DEFAULT_INPUT, help="Input CSV produced by polymarket collector")
    parser.add_argument("--output", type=Path, default=DEFAULT_OUTPUT, help="Output CSV with matched candidates")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    return parser.parse_args(argv)


def ensure_env_ready() -> None:
    if "FEC_LOOKUP_DATABASE_URL" not in os.environ:
        LOGGER.warning(
            "FEC_LOOKUP_DATABASE_URL is not set; lookup.cli commands will likely fail to connect."
        )


def extract_queries(raw: Any) -> List[str]:
    if raw is None:
        return []
    text = str(raw).strip()
    if not text or text.lower() in {"nan", "none", "null"}:
        return []

    parts = [p.strip() for p in re.split(r"[|,]", text) if p.strip()]

    ordered: List[str] = []
    if text:
        ordered.append(text)
    ordered.extend(parts)

    seen = set()
    deduped: List[str] = []
    for item in ordered:
        lowered = item.lower()
        if lowered not in seen:
            seen.add(lowered)
            deduped.append(item)
    return deduped


@lru_cache(maxsize=512)
def lookup_candidates(query: str) -> List[Dict[str, Any]]:
    if not query.strip():
        return []
    cmd = CLI_BASE + [query]
    try:
        completed = subprocess.run(
            cmd,
            capture_output=True,
            check=False,
            text=True,
        )
    except OSError as exc:
        LOGGER.warning("Failed to invoke lookup.cli for '%s': %s", query, exc)
        return []

    if completed.returncode != 0:
        LOGGER.warning(
            "lookup.cli returned non-zero exit status %s for '%s': %s",
            completed.returncode,
            query,
            completed.stderr.strip(),
        )
        return []

    stdout = completed.stdout.strip()
    if not stdout:
        return []
    try:
        payload = json.loads(stdout)
    except json.JSONDecodeError:
        LOGGER.warning("lookup.cli emitted invalid JSON for '%s'", query)
        return []
    if not isinstance(payload, list):
        LOGGER.warning("Unexpected lookup response structure for '%s'", query)
        return []
    return payload


def safe_parse_datetime(value: Any) -> Optional[datetime]:
    if value is None or (isinstance(value, float) and pd.isna(value)):
        return None
    try:
        return pd.to_datetime(value, utc=True).to_pydatetime()
    except (ValueError, TypeError):
        return None


def filter_candidates(
    candidates: Iterable[Dict[str, Any]],
    start_dt: Optional[datetime],
    end_dt: Optional[datetime],
    office_value: Optional[str],
) -> List[CandidateResult]:
    start_year = start_dt.year if start_dt else None
    end_year = end_dt.year if end_dt else None
    office_filter = office_value.lower() if isinstance(office_value, str) else ""
    enforce_office = office_filter in OFFICE_FILTER_SET

    filtered: List[CandidateResult] = []
    seen_keys = set()

    for item in candidates:
        cand_id = item.get("cand_id")
        if not cand_id:
            continue

        cycle = _safe_int(item.get("cycle"))
        election_year = _safe_int(
            item.get("candidate_election_year")
        ) or cycle

        if start_year is not None and election_year is not None and election_year < start_year:
            continue
        if end_year is not None and election_year is not None and election_year > end_year:
            continue

        office_full = _normalize_office(item.get("office_full"))
        office_code = _normalize_office(item.get("office"))
        office_match_value = office_full or office_code

        if enforce_office and office_match_value not in {office_filter, _office_code_from_full(office_filter)}:
            continue

        key = (cand_id, cycle)
        if key in seen_keys:
            continue
        seen_keys.add(key)

        filtered.append(
            CandidateResult(
                cand_id=cand_id,
                normalized_name=item.get("normalized_name"),
                display_name=item.get("display_name"),
                office=item.get("office"),
                office_full=item.get("office_full"),
                cycle=cycle,
                election_year=election_year,
            )
        )

    return filtered


def _safe_int(value: Any) -> Optional[int]:
    try:
        if value is None:
            return None
        ivalue = int(value)
        return ivalue
    except (TypeError, ValueError):
        return None


def _normalize_office(value: Any) -> str:
    if not isinstance(value, str):
        return ""
    return value.strip().lower()


def _office_code_from_full(office: str) -> str:
    lookup = {"president": "p", "senate": "s", "house": "h"}
    return lookup.get(office, "")


def match_row(row: pd.Series, verbose: bool = False) -> List[CandidateResult]:
    queries = extract_queries(row.get("candidate_names"))
    if not queries:
        if verbose:
            LOGGER.info("Row %s has no candidate_names to lookup", row.get("market_id"))
        return []

    aggregated: List[Dict[str, Any]] = []
    for query in queries:
        matches = lookup_candidates(query)
        if verbose:
            LOGGER.info("Query '%s' returned %d candidate(s)", query, len(matches))
        aggregated.extend(matches)

    if not aggregated:
        if verbose:
            LOGGER.info("No candidates returned for row %s", row.get("market_id"))
        return []

    start_dt = safe_parse_datetime(row.get("start_time") or row.get("start_date"))
    end_dt = safe_parse_datetime(row.get("end_time") or row.get("end_date"))
    office_value = row.get("office")

    filtered = filter_candidates(aggregated, start_dt, end_dt, office_value)
    if verbose:
        LOGGER.info("Row %s matched %d candidate(s) after filtering", row.get("market_id"), len(filtered))
    return filtered


def process_dataframe(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:
    original_columns = list(df.columns)
    output_rows: List[Dict[str, Any]] = []

    for idx, row in df.iterrows():
        matches = match_row(row, verbose=verbose)
        base = {col: row[col] for col in original_columns}

        if matches:
            for match in matches:
                record = base.copy()
                record["matched_candidate_id"] = match.cand_id
                record["matched_candidate_name"] = match.name_for_output
                output_rows.append(record)
        else:
            record = base.copy()
            record["matched_candidate_id"] = None
            record["matched_candidate_name"] = None
            output_rows.append(record)

    ordered_columns = original_columns + ["matched_candidate_id", "matched_candidate_name"]
    return pd.DataFrame(output_rows, columns=ordered_columns)


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)
    ensure_env_ready()

    if not args.input.exists():
        raise FileNotFoundError(f"Input CSV not found: {args.input}")

    LOGGER.info("Loading markets from %s", args.input)
    df = pd.read_csv(args.input)
    LOGGER.info("Loaded %d market rows", len(df))

    matched_df = process_dataframe(df, verbose=args.verbose)

    args.output.parent.mkdir(parents=True, exist_ok=True)
    matched_df.to_csv(args.output, index=False)
    LOGGER.info("Wrote %d rows to %s", len(matched_df), args.output)


if __name__ == "__main__":
    main()
